http://blog.csdn.net/programmer_wei/article/details/51941358
给出正确的损失函数(对于线性回归，损失函数为推测值h(x)和观察值y之差的平方和, 损失越小，说明推测值和观察值越接近，拟合程度越高)
梯度下降，要求损失函数有极小值-------------------------如何得到有极小值的损失函数，如何判断损失函数是否有极小值。
对损失函数的每个参数求偏导，对于每个观察值，减去步长*斜率(为什么是减？ 联想二元函数的坐标图)
链接中对参数求偏导时，省略了求和函数，推导后又补上了。

http://blog.csdn.net/ligang_csdn/article/details/53838743
注意sigmoid函数求导的性质
=============================================
梯度下降的本质

1. 梯度下降的意义
机器学习中， 为了拟合出最优的参数， 需要计算出【使损失函数取最小值的参数】（损失函数反映了拟合结果和样本结果的误差程度。当参数取某值时，拟合出来的结果和样本的结果最接近， 这时我们认为这组参数最优【没毛病！】）。
梯度下降算法就是用来计算这组【使损失函数取最小值的参数】。 注意梯度下降其实只能通过迭代的方式接近函数的极小值（局部最优解）， 所以在使用这个方法之前，必须确定这个函数是凸函数（单调时极小值就是最小值【没毛病！】）。

2. 梯度下降的直观理解
设想一个游戏： 你被被蒙着眼睛放置在一个山谷中的任意位置（初始值）， 要寻找山谷的最低点（最小值）。 你是个攀岩高手， 虽然不能飞， 但是能走到山谷的任意一个位置。你手中有个公式， 它能通过前人的经验（样本）估算出你的高度。
解决这个问题最简单粗暴的方法就是【乱走】， 走到一个位置就用公式算一下高度， 记下最低点对应的位置。 然而这种方式效率很低，如果山谷很大，无头苍蝇式的乱走找到最低点的概率极小。
另外一种方式就显得聪明一点。首先把问题分解成这样的步骤： 先找一个正确的方向（梯度），向那个方向移动一段距离（步长）， 用公式计算一下高度。 不停地重复以上步骤， 直到足够接近最低点为止。
显然这种方式的关键就在于如何找到【一个正确的方向】。找到这个正确的方向就要通过对这个公式【求偏导】。我们知道在数学上， 斜率反映了函数在某个点上的变化率， 沿着斜率向下移动是找到极小值最好的方法。 因此， 我们在得到导数之后， 就可以根据上述的步骤， 经过很多次迭代， 找到山谷的最低点了。

3. 感悟
在人生道路上， 你是在乱走还是找对了方向？ 人生道路的公式是什么， 又如何求导？
真实世界是非凸的， 你以为以为找到了最高峰， 结果可能只是在小山坡边上走来走去。
你手中的公式是你仅有的筹码，你只能无条件相信它。它的准确程度， 取决于你到底获得了多少前人的经验。 要得出更加精确的结果， 你需要更多的样本维度（经验的广度）和样本的个数（经验的深度）。 然而太多的样本容易自相矛盾，没有用的维度只会添乱。
方向并不固定， 是需要走一步算一步的。 步子太短， 会花费太多时间在寻找方向上； 步子太长， 多走冤枉路。
